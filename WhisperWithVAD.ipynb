{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Whisper with Silero VAD\n",
        "This notebook combines Whisper with a separate VAD. This improves long-form\n",
        "transcriptions, at the cost of possibly missing a few lines.\n",
        "\n",
        "**Changelog**\n",
        "*   2022-09-29: Added filtering for hallucinations (\"Thank you for watching!\", etc.)"
      ],
      "metadata": {
        "id": "qGS9GFEnOoB_"
      }
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LCqXqFgP2ri0",
        "cellView": "form"
      },
      "source": [
        "#@markdown **GPU check** (you typically want a V100, P100 or T4)\n",
        "!nvidia-smi -L\n",
        "!nvidia-smi"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j9TI-Q6m3qlx",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Mount Google Drive** (skip this if your audio file isn't stored there)\n",
        "from google.colab import drive\n",
        "drive.mount('drive')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "teF-Ut8Z7Gjp",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Setup Whisper**\n",
        "!apt-get install sox libsox-fmt-mp3 libsndfile1 ffmpeg\n",
        "!pip install deepl srt ffmpeg-python\n",
        "!pip install git+https://github.com/openai/whisper.git\n",
        "print(\"Done\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sos9vsxPkIN7",
        "cellView": "form"
      },
      "source": [
        "#@markdown **Run Whisper**\n",
        "\n",
        "# @markdown Required settings (audio_path can be either a local file or a URL):\n",
        "audio_path = \"input.mp3\"  # @param {type:\"string\"}\n",
        "model_size = \"large\"  # @param [\"medium\", \"large\"]\n",
        "language = \"japanese\"  # @param {type:\"string\"}\n",
        "translation_mode = \"End-to-end Whisper (default)\"  # @param [\"End-to-end Whisper (default)\", \"Whisper -> DeepL\", \"No translation\"]\n",
        "# @markdown Advanced settings:\n",
        "deepl_authkey = \"\"  # @param {type:\"string\"}\n",
        "chunk_threshold = 3.0  # @param {type:\"number\"}\n",
        "max_attempts = 1  # @param {type:\"integer\"}\n",
        "\n",
        "import tensorflow as tf\n",
        "import torch\n",
        "import whisper\n",
        "import os\n",
        "import ffmpeg\n",
        "import srt\n",
        "from tqdm import tqdm\n",
        "import datetime\n",
        "import deepl\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "# Configuration\n",
        "assert max_attempts >= 1\n",
        "assert chunk_threshold >= 0.1\n",
        "assert audio_path != \"\"\n",
        "assert language != \"\"\n",
        "if translation_mode == \"End-to-end Whisper (default)\":\n",
        "    task = \"translate\"\n",
        "    run_deepl = False\n",
        "elif translation_mode == \"Whisper -> DeepL\":\n",
        "    task = \"transcribe\"\n",
        "    run_deepl = True\n",
        "elif translation_mode == \"No translation\":\n",
        "    task = \"transcribe\"\n",
        "    run_deepl = False\n",
        "else:\n",
        "    raise ValueError(\"Invalid translation mode\")\n",
        "\n",
        "if \"http://\" in audio_path or \"https://\" in audio_path:\n",
        "    print(\"Downloading audio...\")\n",
        "    urllib.request.urlretrieve(audio_path, \"input_file\")\n",
        "    audio_path = \"input_file\"\n",
        "\n",
        "print(\"Encoding audio...\")\n",
        "if not os.path.exists(\"vad_chunks\"):\n",
        "    os.mkdir(\"vad_chunks\")\n",
        "ffmpeg.input(audio_path).output(\n",
        "    \"vad_chunks/silero_temp.wav\",\n",
        "    ar=\"16000\",\n",
        "    ac=\"1\",\n",
        "    acodec=\"pcm_s16le\",\n",
        "    map_metadata=\"-1\",\n",
        "    fflags=\"+bitexact\",\n",
        ").overwrite_output().run(quiet=True)\n",
        "\n",
        "print(\"Running VAD...\")\n",
        "model, utils = torch.hub.load(\n",
        "    repo_or_dir=\"snakers4/silero-vad\", model=\"silero_vad\", onnx=False\n",
        ")\n",
        "\n",
        "(get_speech_timestamps, save_audio, read_audio, VADIterator, collect_chunks) = utils\n",
        "\n",
        "# Generate VAD timestamps\n",
        "VAD_SR = 16000\n",
        "wav = read_audio(\"vad_chunks/silero_temp.wav\", sampling_rate=VAD_SR)\n",
        "t = get_speech_timestamps(wav, model, sampling_rate=VAD_SR)\n",
        "\n",
        "# Add a bit of padding, and remove small gaps\n",
        "for i in range(len(t)):\n",
        "    t[i][\"start\"] = max(0, t[i][\"start\"] - 3200)  # 0.2s head\n",
        "    t[i][\"end\"] = min(wav.shape[0] - 16, t[i][\"end\"] + 20800)  # 1.3s tail\n",
        "    if i > 0 and t[i][\"start\"] < t[i - 1][\"end\"]:\n",
        "        t[i][\"start\"] = t[i - 1][\"end\"]  # Remove overlap\n",
        "\n",
        "# If breaks are longer than chunk_threshold seconds, split into a new audio file\n",
        "# This'll effectively turn long transcriptions into many shorter ones\n",
        "u = [[]]\n",
        "for i in range(len(t)):\n",
        "    if i > 0 and t[i][\"start\"] > t[i - 1][\"end\"] + (chunk_threshold * VAD_SR):\n",
        "        u.append([])\n",
        "    u[-1].append(t[i])\n",
        "\n",
        "# Merge speech chunks\n",
        "for i in range(len(u)):\n",
        "    save_audio(\n",
        "        \"vad_chunks/\" + str(i) + \".wav\",\n",
        "        collect_chunks(u[i], wav),\n",
        "        sampling_rate=VAD_SR,\n",
        "    )\n",
        "os.remove(\"vad_chunks/silero_temp.wav\")\n",
        "\n",
        "# Convert timestamps to seconds\n",
        "for i in range(len(u)):\n",
        "    time = 0.0\n",
        "    offset = 0.0\n",
        "    for j in range(len(u[i])):\n",
        "        u[i][j][\"start\"] /= VAD_SR\n",
        "        u[i][j][\"end\"] /= VAD_SR\n",
        "        u[i][j][\"chunk_start\"] = time\n",
        "        time += u[i][j][\"end\"] - u[i][j][\"start\"]\n",
        "        u[i][j][\"chunk_end\"] = time\n",
        "        if j == 0:\n",
        "            offset += u[i][j][\"start\"]\n",
        "        else:\n",
        "            offset += u[i][j][\"start\"] - u[i][j - 1][\"end\"]\n",
        "        u[i][j][\"offset\"] = offset\n",
        "\n",
        "# Run Whisper on each audio chunk\n",
        "print(\"Running Whisper...\")\n",
        "model = whisper.load_model(model_size)\n",
        "subs = []\n",
        "lines = []\n",
        "segment_info = []\n",
        "punct_match = [\"。\", \"、\", \",\", \".\", \"〜\", \"！\", \"!\", \"？\", \"?\", \"-\"]\n",
        "sub_index = 1\n",
        "suppress_low = [\n",
        "    \"Thank you\",\n",
        "    \"Thanks for\",\n",
        "    \"ike and \",\n",
        "    \"Bye.\",\n",
        "    \"Bye!\",\n",
        "    \"Bye bye!\",\n",
        "    \"lease sub\",\n",
        "    \"The end.\",\n",
        "]\n",
        "suppress_high = [\n",
        "    \"ubscribe\",\n",
        "    \"my channel\",\n",
        "    \"the channel\",\n",
        "    \"our channel\",\n",
        "    \"ollow me on\",\n",
        "    \"for watching\",\n",
        "    \"for your viewing\",\n",
        "    \"r viewing\",\n",
        "    \"Amara\",\n",
        "    \"next video\",\n",
        "    \"full video\",\n",
        "    \"ranslation by\",\n",
        "    \"ranslated by\",\n",
        "    \"ee you next week\",\n",
        "]\n",
        "for i in tqdm(range(len(u))):\n",
        "    for x in range(max_attempts):\n",
        "        result = model.transcribe(\n",
        "            \"vad_chunks/\" + str(i) + \".wav\", task=task, language=language\n",
        "        )\n",
        "        # Break if result doesn't end with severe hallucinations\n",
        "        if len(result[\"segments\"]) == 0:\n",
        "            break\n",
        "        elif result[\"segments\"][-1][\"end\"] < u[i][-1][\"chunk_end\"] + 10.0:\n",
        "            break\n",
        "        elif x+1 < max_attempts:\n",
        "            print(\"Retrying chunk\", i)\n",
        "    for r in result[\"segments\"]:\n",
        "        # Skip audio timestamped after the chunk has ended\n",
        "        if r[\"start\"] > u[i][-1][\"chunk_end\"]:\n",
        "            continue\n",
        "        # Reduce log probability for certain words/phrases\n",
        "        for s in suppress_low:\n",
        "            if s in r[\"text\"]:\n",
        "                r[\"avg_logprob\"] -= 0.15\n",
        "        for s in suppress_high:\n",
        "            if s in r[\"text\"]:\n",
        "                r[\"avg_logprob\"] -= 0.35\n",
        "        # Keep segment info for debugging\n",
        "        del r[\"tokens\"]\n",
        "        segment_info.append(r)\n",
        "        # Skip if log prob is low or no speech prob is high\n",
        "        if r[\"avg_logprob\"] < -1.0 or r[\"no_speech_prob\"] > 0.7:\n",
        "            continue\n",
        "        # Set start timestamp\n",
        "        start = r[\"start\"] + u[i][0][\"offset\"]\n",
        "        for j in range(len(u[i])):\n",
        "            if (\n",
        "                r[\"start\"] >= u[i][j][\"chunk_start\"]\n",
        "                and r[\"start\"] <= u[i][j][\"chunk_end\"]\n",
        "            ):\n",
        "                start = r[\"start\"] + u[i][j][\"offset\"]\n",
        "                break\n",
        "        # Prevent overlapping subs\n",
        "        if len(subs) > 0:\n",
        "            last_end = datetime.timedelta.total_seconds(subs[-1].end)\n",
        "            if last_end > start:\n",
        "                subs[-1].end = datetime.timedelta(seconds=start)\n",
        "        # Set end timestamp\n",
        "        end = u[i][-1][\"end\"] + 0.5\n",
        "        for j in range(len(u[i])):\n",
        "            if r[\"end\"] >= u[i][j][\"chunk_start\"] and r[\"end\"] <= u[i][j][\"chunk_end\"]:\n",
        "                end = r[\"end\"] + u[i][j][\"offset\"]\n",
        "                break\n",
        "        # Add to SRT list\n",
        "        subs.append(\n",
        "            srt.Subtitle(\n",
        "                index=sub_index,\n",
        "                start=datetime.timedelta(seconds=start),\n",
        "                end=datetime.timedelta(seconds=end),\n",
        "                content=r[\"text\"].strip(),\n",
        "            )\n",
        "        )\n",
        "        # Add punctuation to DeepL input (improves translation)\n",
        "        lines.append(r[\"text\"].strip())\n",
        "        if lines[-1][-1] not in punct_match:\n",
        "            if language.lower() == \"japanese\" and task == \"transcribe\":\n",
        "                lines[-1] += \"。\"\n",
        "            else:\n",
        "                lines[-1] += \".\"\n",
        "        sub_index += 1\n",
        "with open(\"segment_info.json\", \"w\", encoding=\"utf8\") as f:\n",
        "    json.dump(segment_info, f, indent=4)\n",
        "\n",
        "# DeepL translation\n",
        "if run_deepl:\n",
        "    print(\"Translating...\")\n",
        "    translator = deepl.Translator(deepl_authkey)\n",
        "    if language.lower() == \"japanese\":\n",
        "        result = translator.translate_text(lines, source_lang=\"JA\", target_lang=\"EN-US\")\n",
        "    else:\n",
        "        result = translator.translate_text(lines, target_lang=\"EN-US\")\n",
        "    for i, translation in enumerate(result):\n",
        "        subs[i].content = translation.text.strip()\n",
        "\n",
        "# Write SRT file\n",
        "out_path = os.path.splitext(audio_path)[0] + \".srt\"\n",
        "with open(out_path, \"w\", encoding=\"utf8\") as f:\n",
        "    f.write(srt.compose(subs))\n",
        "print(\"\\nDone! Subs written to\", out_path)\n"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
